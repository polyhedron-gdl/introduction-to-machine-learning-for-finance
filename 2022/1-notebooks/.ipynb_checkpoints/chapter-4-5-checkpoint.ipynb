{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d22089",
   "metadata": {},
   "source": [
    "# Model Evaluation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00543760",
   "metadata": {},
   "source": [
    ">**TODO** \n",
    "> - Completare la parte sulle pipelines introducendo esempio di Lewinson\n",
    "> - Introdurre anche esempi sulla regressione logistica con credito"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f71fb",
   "metadata": {},
   "source": [
    "In the previous chapters, you learned about the essential machine learning algorithms for classification and how to get our data into shape before we feed it into those algorithms. Now, it's time to learn about the best practices of building good machine learning models by fine-tuning the algorithms and evaluating the performance of the models. In this chapter, we will learn how to do the following:\n",
    "- Assess the performance of machine learning models\n",
    "- Diagnose the common problems of machine learning algorithms\n",
    "- Fine-tune machine learning models\n",
    "- Evaluate predictive models using different performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f915f83",
   "metadata": {},
   "source": [
    "## Combining transformers and estimators in a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e27dc7",
   "metadata": {},
   "source": [
    "The make_pipeline function takes an arbitrary number of scikit-learn transformers (objects that support the fit and transform methods as input), followed by a scikit- learn estimator that implements the fit and predict methods.\n",
    "\n",
    "We can think of a scikit-learn Pipeline as a meta-estimator or wrapper around those individual transformers and estimators. If we call the fit method of Pipeline, the data will be passed down a series of transformers via fit and transform calls\n",
    "on these intermediate steps until it arrives at the estimator object (the final element\n",
    "in a pipeline). The estimator will then be fitted to the transformed training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27480da7",
   "metadata": {},
   "source": [
    "**INSERIRE ESEMPIO DI LEWINSON SU CREDIT DEFAULT CARD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c92a87",
   "metadata": {},
   "source": [
    "The make_pipeline function takes an arbitrary number of scikit-learn transformers\n",
    "(objects that support the fit and transform methods as input), followed by a scikitlearn\n",
    "estimator that implements the fit and predict methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15840214",
   "metadata": {},
   "source": [
    "## Model Performance and Cross-Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e73d69",
   "metadata": {},
   "source": [
    "One of the key steps in building a machine learning model is to estimate its\n",
    "performance on data that the model hasn't seen before. Let's assume that we fit our\n",
    "model on a training dataset and use the same data to estimate how well it performs on\n",
    "new data. \n",
    "\n",
    "We remember from that a model can suffer from\n",
    "**underfitting (high bias)** if the model is too simple, or it can **overfit** the training data\n",
    "**(high variance)** if the model is too complex for the underlying training data.\n",
    "To find an acceptable *bias-variance tradeoff*, we need to evaluate our model\n",
    "carefully. In this section, you will learn about the common **cross-validation**\n",
    "techniques holdout cross-validation and k-fold cross-validation, which can help\n",
    "us to obtain reliable estimates of the model's generalization performance, that is,\n",
    "how well the model performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37940789",
   "metadata": {},
   "source": [
    "### Holdout Method\n",
    "\n",
    "Using the holdout method, we\n",
    "split our initial dataset into separate training and test datasets—the former is used\n",
    "for model training, and the latter is used to estimate its generalization performance.\n",
    "However, in typical machine learning applications, we are also interested in tuning\n",
    "and comparing different parameter settings to further improve the performance for\n",
    "making predictions on unseen data. This process is called model selection, with\n",
    "the name referring to a given classification problem for which we want to select\n",
    "the optimal values of tuning parameters (also called hyperparameters). However, if\n",
    "we reuse the same test dataset over and over again during model selection, it will\n",
    "become part of our training data and thus the model will be more likely to overfit.\n",
    "\n",
    "A better way of using the holdout method for model selection is to separate the\n",
    "data into three parts: a training dataset, a validation dataset, and a test dataset.\n",
    "The training dataset is used to fit the different models, and the performance on the\n",
    "validation dataset is then used for the model selection. The advantage of having\n",
    "a test dataset that the model hasn't seen before during the training and model\n",
    "selection steps is that we can obtain a less biased estimate of its ability to generalize\n",
    "to new data. The following figure illustrates the concept of holdout cross-validation,\n",
    "where we use a validation dataset to repeatedly evaluate the performance of the\n",
    "model after training using different hyperparameter values. Once we are satisfied\n",
    "with the tuning of hyperparameter values, we estimate the model's generalization\n",
    "performance on the test dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c09d4",
   "metadata": {},
   "source": [
    "![chapter-4-5_pic_0.png](./pic/chapter-4-5_pic_0.png)\n",
    "*From S. Raschka et al. (see Reference)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a203ffc",
   "metadata": {},
   "source": [
    "### k-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1986e",
   "metadata": {},
   "source": [
    "In k-fold cross-validation, we randomly split the training dataset into k folds without\n",
    "replacement, where k – 1 folds are used for the model training, and one fold is used\n",
    "for performance evaluation. This procedure is repeated k times so that we obtain k\n",
    "models and performance estimates.\n",
    "\n",
    "We then calculate the average performance of the models based on the different,\n",
    "independent test folds to obtain a performance estimate that is less sensitive\n",
    "to the sub-partitioning of the training data compared to the holdout method.\n",
    "Typically, we use k-fold cross-validation for model tuning, that is, finding the\n",
    "optimal hyperparameter values that yield a satisfying generalization performance,\n",
    "which is estimated from evaluating the model performance on the test folds.\n",
    "Once we have found satisfactory hyperparameter values, we can retrain the model\n",
    "on the complete training dataset and obtain a final performance estimate using the\n",
    "independent test dataset. The rationale behind fitting a model to the whole training\n",
    "dataset after k-fold cross-validation is that providing more training examples to\n",
    "a learning algorithm usually results in a more accurate and robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5229e79",
   "metadata": {},
   "source": [
    "![chapter-4-5_pic_1.png](./pic/chapter-4-5_pic_1.png)\n",
    "*From S. Raschka et al. (see Reference)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e549492",
   "metadata": {},
   "source": [
    "## Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4df9bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43687d3b",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters via grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e66f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00221945",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc5a88a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e5059b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba9dc3d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "200502a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c951a06d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f2d554",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e18ae",
   "metadata": {},
   "source": [
    "*Eryk Lewinson*, \"**Python For Finance Cookbook**\",  Packt Publishing (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd81eb",
   "metadata": {},
   "source": [
    "*Sebastian Raschka and Vahid Mirjalili* \"**Machine Learning with Python**\", 3rd edition, Packt Publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaac585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
