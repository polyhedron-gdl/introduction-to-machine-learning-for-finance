{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/qf-workshop-2021/introduction-to-machine-learning/blob/main/1_notebooks/introduction-to-machine-learning.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions \n",
    "\n",
    "\n",
    "### Linear Cost Function \n",
    "\n",
    "In Machine Learning a cost function or loss function is used to represent how far away a mathematical model is from the real data. One adjusts the mathematical model, usually by varying parameters within the model, so as to minimize the cost function. \n",
    "\n",
    "Let's take for example the simple case of a linear fitting. We want to find a relationship of the form \n",
    "\n",
    "\\begin{equation}\n",
    "y=\\theta_0 +\\theta_1x\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\theta$s are the parameters that we want to find to give us the best fit to the data. We call this linear function $h_\\theta(x)$ to emphasize the dependence on both the variable $x$ and the two parameters $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "\n",
    "We want to measure how far away the data, the $y^{(n)}$s, are from the function $h_\\theta(x)$. A common way to do this is via the quadratic *cost function*\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{\\theta}) = \\frac{1}{2N} \\sum\\limits_{n=1}^N \\left[ h_\\theta \\left( x^{(n)} \\right) - y^{(n)} \\right]^2\n",
    "\\label{eq:ols}\n",
    "\\end{equation}\n",
    "\n",
    "This is called *Ordinary Least Squares*.\n",
    "\n",
    "In this case, the minimum is easily find analitically, differentiate $\\eqref{eq:ols}$ with respect to both $\\theta$s and set the result to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{lcl} \n",
    "\\frac{\\partial J}{\\partial \\theta_0} & = & \\sum\\limits_{n=1}^N \\left( \\theta_0 + \\theta_1 x^{(n)} - y^{(n)} \\right) = 0 \n",
    "\\\\ \n",
    "\\frac{\\partial J}{\\partial \\theta_1} & = & \\sum\\limits_{n=1}^N x^{(n)} \\left( \\theta_0 + \\theta_1 x^{(n)} - y^{(n)} \\right) = 0 \n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "The solution is trivially obtained for both $\\theta$s\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{lcl} \n",
    "\\theta_0 = \\frac{\\left(\\sum y \\right) \\left(\\sum x^2 \\right) -\\left(\\sum x \\right) \\left(\\sum xy \\right) }{N\\left(\\sum x^2 \\right) \\left(\\sum x \\right)^2 } \n",
    "\\\\ \n",
    "\\theta_1 = \\frac{N\\left(\\sum xy \\right) - \\left(\\sum y \\right)\\left(\\sum x \\right)}{N\\left(\\sum x^2 \\right) \\left(\\sum x \\right)^2 }\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The scheme works as follow: start with an initial guess for each parameter $\\theta_k$. Then move $\\theta_k$ in the direction of the slope:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_k^{new} =\\theta_k^{old}+\\beta \\frac{\\partial J}{\\partial \\theta_k}\n",
    "\\end{equation}\n",
    "\n",
    "**Update all $\\theta_k$ simultaneously** and repet until convergence. Here $\\beta$ is a *learning factor* that governs how far you move. if $\\beta$ is too small it will take a long time to converge, if too large it will overshoot and might not converge at all. \n",
    "\n",
    "The loss function $J$ is a function of all of the data points. In the above description of gradient descent we have used all of the data points simultaneously. This is called *batch gradient* descent. But rather than use all of the data in the parameter updating we can use a technique called *stochastic gradient descent*. This is like batch gradient descent except that you only update using *one* of the data points each time. And that data point is chosen randomly.\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{\\theta}) = \\sum\\limits_{n=1}^N J_n(\\mathbf{\\theta})\n",
    "\\end{equation}\n",
    "\n",
    "Stochastic gradient descent means pick an *n* at random and then update according to \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_k^{new} =\\theta_k^{old}+\\beta \\frac{\\partial J_n}{\\partial \\theta_k}\n",
    "\\end{equation}\n",
    "\n",
    "Repeat, picking another data point at random, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter in Gradient Descent is the size of the steps, determined by\n",
    "the learning rate hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"pic_50.png\" width=\"600\"/>\n",
    "</div>\n",
    "-->\n",
    "![caption](pic_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning rate is too small, then the algorithm\n",
    "will have to go through many iterations to converge, which will take a long time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"pic_51.png\" width=\"600\"/>\n",
    "</div>\n",
    "-->\n",
    "![caption](pic_51.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... on the other hand, if the learning rate is too high, you might jump across the valley. This might make the algorithm diverge failing to find a good solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"pic_52.png\" width=\"600\"/>\n",
    "</div>\n",
    "-->\n",
    "![caption](pic_52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "331px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
